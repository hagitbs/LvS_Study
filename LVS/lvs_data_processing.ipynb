{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "import configparser\n",
    "import argparse \n",
    "from helpers import read\n",
    " \n",
    "import bottleneck as bn\n",
    "from LPA import Corpus, sockpuppet_distance\n",
    "from math import floor\n",
    "from scipy.spatial.distance import cdist, cityblock\n",
    "import matplotlib.pyplot as plt\n",
    "from visualize import sockpuppet_matrix, timeline\n",
    "alt.data_transformers.disable_max_rows()\n",
    "from unpivot_utils import unpivot_wide_dataframe\n",
    "from unpivot_utils import unpivot_dataframe \n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path,columns_to_remove):\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Data successfully loaded from {file_path}\")\n",
    "        print(f\"DataFrame shape: {df.shape}\") \n",
    "        #print 5 rows of the DataFrame\n",
    "        # remove the columns 'E1', 'E2', 'E3', 'E4' from the dataframe  \n",
    "        #columns_to_remove = ['E1', 'E2', 'E3', 'E4']\n",
    "        for col in columns_to_remove:\n",
    "            if col in df.columns:\n",
    "                print(f\"Removing column '{col}' from the DataFrame\")\n",
    "                df = df.drop(columns=[col])\n",
    "        # remove the column 'E5'  from the dataframe    \n",
    "        if 'E5' in df.columns:\n",
    "            print(f\"Removing column 'E5' from the DataFrame\")\n",
    "            df = df.drop(columns=['E5']) \n",
    "        print(df.head(5))\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None  # Important: Return None on error\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "def unpivot_data(df, agg_column , var_name , value_name ,ignore_columns, processing_type,file_path2):  \n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Skipping unpivot_data.\")\n",
    "        return None \n",
    "    try:\n",
    "        if processing_type == 'full':\n",
    "            df_melted=unpivot_dataframe(df, ignore_columns, agg_column, var_name, value_name, file_path2)   \n",
    "\n",
    "             \n",
    "        elif processing_type == 'wide_unpivot': \n",
    "            ID_COLUMNS = ['element']\n",
    "\n",
    "            #unpivot_wide_csv_by_row('/Users/hagitbenshoshan/Documents/PHD/LVS_Code/LVS/data/alon/Allondata.csv',melted_df, ID_COLUMNS)\n",
    "            df_melted=unpivot_wide_dataframe(df,ID_COLUMNS,value_name='frequency_in_document', var_name='document')\n",
    "            # --- 4. Display the Results ---\n",
    "            print(\"\\n--- Resulting Long DataFrame ---\")\n",
    "            print(\"Shape:\", df_melted.shape)\n",
    "            print(\"First 10 rows:\")\n",
    "            print(df_melted.head(10))\n",
    "            \n",
    "        else:  \n",
    "            df_melted=df \n",
    "            df_melted = df_melted.reset_index() \n",
    "\n",
    "            \n",
    "        print(f\"agg_column: {agg_column}, var_name: {var_name}, value_name: {value_name}, ignore_columns: {ignore_columns}\")\n",
    "\n",
    "        df_melted_grouped = df_melted.groupby([agg_column, var_name])[value_name].sum().reset_index()\n",
    "\n",
    "        # Calculate the total deaths per year\n",
    "        df_melted_grouped['Total_Per_Agg'] = df_melted_grouped.groupby(agg_column)[value_name].transform('sum')\n",
    "        # Calculate the relative deaths\n",
    "        # additional column to calculate the relative [Optional]  \n",
    "\n",
    "        if file_path2 == 'None':\n",
    "                            \n",
    "            df_melted_grouped['frequency_in_document'] = df_melted_grouped[value_name] / df_melted_grouped['Total_Per_Agg']\n",
    "        else:   \n",
    "            \n",
    "            df2 = pd.read_csv(file_path2)   \n",
    "            # Merge with the population data\n",
    "            df_melted_grouped  = pd.merge(df_melted_grouped, df2, left_on=agg_column, right_on='Year', how='inner') \n",
    "            df_melted_grouped['frequency_in_document'] = df_melted_grouped[value_name] / df_melted_grouped['Population']\n",
    "\n",
    "        # Rename\n",
    "        df_melted_grouped = df_melted_grouped.rename(columns={agg_column:'document',\n",
    "                                var_name: 'element'}) \n",
    "        print(f\"Unpivoted data shape: {df_melted_grouped.head(5)}\")\n",
    "        return df_melted_grouped  \n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column not found: {e}.  Check your 'agg_column', 'var_name', 'value_name', and 'ignore_columns' parameters.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during unpivoting: {e}\")\n",
    "        return None\n",
    "def clean_data(df,columns_to_keep, short_names, dataset):\n",
    "    \"\"\"Cleans the DataFrame (e.g., keep only relevant columns , handles missing values, data type conversions).\"\"\"\n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Skipping clean_data.\")\n",
    "        return None, None\n",
    "    try:\n",
    "        #clean elements that always = 0 in all the documents \n",
    "        print(\"Cleaning data...\")\n",
    "        # Group by 'element' and sum the frequency across all documents\n",
    "        non_zero_elements = df.groupby('element')['frequency_in_document'].sum()\n",
    "        #print (f\"Non-zero elements: {non_zero_elements}\")\n",
    "        #print zero elements\n",
    "        #print (f\"Zero elements: {non_zero_elements[non_zero_elements == 0]}\")\n",
    "        # save the zero elements to a file\n",
    "        non_zero_elements[non_zero_elements == 0].to_csv(f\"results/{dataset}/zero_elements.csv\")\n",
    "        #save the non zero elements to a file\n",
    "        non_zero_elements[non_zero_elements > 0].to_csv(f\"results/{dataset}/non_zero_elements.csv\")\n",
    "        # Keep only elements with a non-zero total frequency\n",
    "        non_zero_elements = non_zero_elements[non_zero_elements > 0].index\n",
    "        # Filter the original DataFrame to keep only those elements\n",
    "        filtered_df = df[df['element'].isin(non_zero_elements)]\n",
    "        df= filtered_df\n",
    "        #print(f\"Filtered df 50: {df.head(50)}\")\n",
    "        df_cleaned = df.dropna()\n",
    "        # Keep only the relevant columns\n",
    "        df_cleaned = df_cleaned[columns_to_keep] \n",
    "        entity_code_df = None\n",
    "            # Shorten the element names\n",
    "        unique_elements = df['element'].unique()\n",
    "        #print(f\"Unique elements: {unique_elements}\")\n",
    "        if short_names=='True':\n",
    "            element_to_code = { element: f'E{i}' for i,  element  in enumerate(unique_elements) }\n",
    "        else:\n",
    "            # Create a mapping from element names to codes\n",
    "            element_to_code = {element: element for i, element in enumerate(unique_elements)}\n",
    "            \n",
    "        df_cleaned['element'] = df_cleaned['element'].map(element_to_code)  \n",
    "        # Create a DataFrame from the dictionary\n",
    "        entity_code_df = pd.DataFrame(list(element_to_code.items()), columns=['element_name', 'element']) \n",
    "        #df_cleaned['amount'] = pd.to_numeric(df_cleaned['amount'], errors='coerce') \n",
    "        return df_cleaned, entity_code_df\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column not found: {e}. Check your 'columns_to_keep' parameter.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data cleaning: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "def filter_data(df, column, condition):\n",
    "    \"\"\"Filters the DataFrame based on a condition.\"\"\"\n",
    "    print(f\"Filtering data where {column} {condition}\")\n",
    "    return df[df[column] > condition]\n",
    "\n",
    "def calculate_summary(df, group_by_column, aggregation):\n",
    "    \"\"\"Calculates summary statistics on the DataFrame.\"\"\"\n",
    "    print(f\"Calculating summary by {group_by_column}...\")\n",
    "    return df.groupby(group_by_column).agg(aggregation)\n",
    "\n",
    "def save_results(df,entity_code_df, output_path,output_dic):\n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Skipping save_results.\")\n",
    "        return\n",
    "\n",
    "    \"\"\"Saves the processed DataFrames to a CSV file.\"\"\"\n",
    "    print(f\"Saving results to: {output_path} and {output_dic}\") \n",
    "    try: \n",
    "        print(f\"Saving results to: {output_path} and {output_dic}\")\n",
    "        df.to_csv(output_path, index=False)  # Don't include the index\n",
    "        print(f\"Data successfully saved to {output_path}\")\n",
    "\n",
    "        if output_dic:\n",
    "            #  Create a DataFrame from the dictionary and save it.  Important for consistent structure.\n",
    "            entity_code_df.to_csv(output_dic, index=False)            \n",
    "            print(f\"Dictionary successfully saved to {output_path.replace('.csv', '_dict.csv')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def generate_signatures(df, entity_code_df, sig_file, dataset,graph,top,sig_length,var_name,value_name):\n",
    "    \"\"\"\n",
    "    Generates and saves document signatures, along with related analyses and visualizations.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing document data.\n",
    "        entity_code_df (pd.DataFrame, optional): DataFrame mapping entity codes to names.\n",
    "        sig_file (str, optional): Path to save the signature DataFrame.\n",
    "        dataset (str): Name of the dataset for output directory.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Skipping generate_signatures.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Create results directory if it doesn't exist\n",
    "        import os\n",
    "        os.makedirs(f\"results/{dataset}\", exist_ok=True)\n",
    "        print (f\"sig_file: {sig_file}, dataset: {dataset}, graph: {graph}, top: {top}, sig_length: {sig_length}, var_name: {var_name}, value_name: {value_name}\")\n",
    "\n",
    "        corpus = Corpus(df, \"document\", \"element\", \"frequency_in_document\")\n",
    "        dvr = corpus.create_dvr(equally_weighted=True) # Create Document Vector Representation (DVR)\n",
    "        dvr.to_csv(f\"results/{dataset}/dvr.csv\")\n",
    "        top = int(top)\n",
    "        sig_length = int(sig_length)\n",
    "\n",
    "        sigs = corpus.create_signatures(distance=\"JSD\",sig_length=sig_length, most_significant=top,prevalent=0.1) #Hagit check if this is the right distance\n",
    "\n",
    "        #  Saving top N changed elements\n",
    "        sigs[1].to_csv(f\"results/{dataset}/top_{top}_most_changed.csv\")\n",
    "        sig = pd.DataFrame(sigs[1])\n",
    "\n",
    "        # Rename columns based on entity_code_df if provided\n",
    "        if entity_code_df is not None:\n",
    "            entity_code_to_name = entity_code_df.set_index(\"element\")[\"element_name\"].to_dict()\n",
    "            new_columns = [\n",
    "                entity_code_to_name.get(col, col) for col in sig.columns\n",
    "            ]  # Use get() for safety\n",
    "            sig.columns = new_columns\n",
    "            sig.to_csv(f\"results/{dataset}/top_{top}_most_changed_real_names.csv\")\n",
    "\n",
    "        # Save signatures if sig_file is provided\n",
    "        if sig_file:\n",
    "            ndf = pd.DataFrame(sigs[0])\n",
    "            ndf.to_csv(sig_file, index=True)\n",
    "            print(f\"Signatures successfully saved to {sig_file}\")\n",
    "            #save the signatures with real names\n",
    "            if entity_code_df is not None:\n",
    "                ndf.columns = [entity_code_to_name.get(col, col) for col in ndf.columns]\n",
    "                ndf.to_csv(sig_file.replace('.csv', '_real_names.csv'), index=True)\n",
    "                print(f\"Signatures with real names successfully saved to {sig_file.replace('.csv', '_real_names.csv')}\")\n",
    "        else:\n",
    "            print(\"No signature file provided, skipping signature saving.\")\n",
    "\n",
    "        # split the ndf DataFrame  to several dataframes   , by the column name   \n",
    "        output_dir = f\"results/{dataset}/split_dataframes\"\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist \n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # Iterate over each row of the DataFrame using iterrows()\n",
    "        # This method yields both the index and the row (as a Series)\n",
    "        for index, row in ndf.iterrows():\n",
    "            # Convert the row (which is a pandas Series) to a DataFrame\n",
    "            # .to_frame() converts the Series to a DataFrame with the original Series index as the new DataFrame's index\n",
    "            # We can provide a column name, for instance, using the original index\n",
    "            \n",
    "            row = row[row.notnull()]\n",
    "\n",
    "            row_df = row.to_frame(name=f'row_{index}_data')\n",
    "\n",
    "            # Pivot the result. For a single row DataFrame, transposing it achieves the desired pivoted effect.\n",
    "            pivoted_df = row_df \n",
    "\n",
    "\n",
    "            # Define a unique filename for each new CSV file\n",
    "            file_name = os.path.join(output_dir, f'row_{index}.csv') \n",
    "\n",
    "            # If the DataFrame is empty after dropping NaN columns, skip saving\n",
    "            if pivoted_df.empty:    \n",
    "                print(f\"Row {index} has no data to save, skipping.\")\n",
    "                continue    \n",
    "\n",
    "            # Save the pivoted DataFrame to a new CSV file.\n",
    "            # The column names from the original DataFrame will be preserved as the header.\n",
    "            # Select rows where the second column (index 1) is not null\n",
    "\n",
    "            pivoted_df = pivoted_df[pivoted_df.iloc[:, 0].notna()]\n",
    "            pivoted_sorted_desc = pivoted_df.sort_values(by=pivoted_df.columns[0], ascending=False)\n",
    "            #pivoted_sorted_desc['col1_numeric'] = pd.to_numeric(pivoted_sorted_desc[pivoted_sorted_desc.columns[0]], errors='coerce')\n",
    "            #pivoted_sorted_desc= pivoted_sorted_desc.sort_values(by=pivoted_sorted_desc.columns[1].abs(), ascending=False)\n",
    "            pivoted_sorted_desc.to_csv(file_name, index=True)\n",
    "            \n",
    "            # Convert column[0] to numeric. 'coerce' will turn invalid parsing into NaN.\n",
    "            ''' \n",
    "            pivoted_df['col1_numeric'] = pd.to_numeric(pivoted_df[pivoted_df.columns[0]], errors='coerce')\n",
    "\n",
    "            # Drop rows where conversion failed (optional, depending on how you want to handle errors)\n",
    "            pivoted_df.dropna(subset=['col1_numeric'], inplace=True)\n",
    "\n",
    "            # Sort rx_filtered by the absolute value of the newly created numeric column in descending order\n",
    "            rx_sorted_abs_desc = pivoted_df.sort_values(by=pivoted_df['col1_numeric'].abs(), ascending=False)\n",
    "\n",
    "            # Drop the temporary numeric column if you don't need it\n",
    "            rx_sorted_abs_desc = rx_sorted_abs_desc.drop(columns=['col1_numeric'])\n",
    "            rx_sorted_abs_desc.to_csv(file_name, index=True)\n",
    "            '''\n",
    "\n",
    "            print(f\"Saved pivoted data for row {index} to '{file_name}'\") \n",
    "\n",
    "           # Save element list\n",
    "        with open(f\"results/{dataset}/list.txt\", \"w\") as f:\n",
    "            for item in sigs[0]:\n",
    "                f.write(f\"{item}\\n\")\n",
    "        # save list into dataframe  \n",
    "        df_list = pd.DataFrame(sigs[0])\n",
    "        print(f\"Element list saved to results/{dataset}/list.txt\")\n",
    "        #print(df_list.head(5))\n",
    "        #pivot_the_list = df_list.melt(var_name='element', value_name='value', ignore_index=False)\n",
    "        print(df_list.columns)\n",
    "        #print(df_list.head(5))\n",
    "        pivot_the_list = df_list.melt(var_name=var_name, value_name=value_name, ignore_index=False)\n",
    "        pivot_the_list = pivot_the_list.reset_index().rename(columns={'index': 'document'})\n",
    "        df_list = pivot_the_list.dropna().reset_index(drop=True)   \n",
    "        df_list.to_csv(f\"results/{dataset}/list.csv\", index=True)\n",
    "        print(f\"Element list saved to results/{dataset}/list.csv\")      \n",
    "\n",
    "        # Sockpuppet analysis\n",
    "        if graph == 'True':\n",
    "            ecorpus = Corpus(df)\n",
    "            ecorpus_dvr = ecorpus.create_dvr(equally_weighted=True)  # Corrected variable name\n",
    "            esigs = ecorpus.create_signatures(distance=\"JSD\")\n",
    "            espd = sockpuppet_distance(ecorpus, ecorpus, heuristic=False, distance=\"euclidean\")\n",
    "            chart = sockpuppet_matrix(espd)\n",
    "            if chart is not None:\n",
    "                try:\n",
    "                    chart.save(f\"results/{dataset}/sockpuppet_distance_matrix.png\", scale_factor=4.0)\n",
    "                    print(f\"Sockpuppet distance matrix chart saved to results/{dataset}/sockpuppet_distance_matrix.png\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving sockpuppet distance matrix chart: {e}\")\n",
    "\n",
    "            espd.to_csv(f\"results/{dataset}/sockpuppet_distance_matrix.csv\", index=False)\n",
    "\n",
    "            # Top 10 distances chart\n",
    "            try:\n",
    "                top_changing = sig[sig.sum(0).abs().sort_values(ascending=False).head(10).index]\n",
    "                chart = (\n",
    "                    alt.Chart(\n",
    "                        top_changing.reset_index()\n",
    "                        .melt(id_vars=\"index\")\n",
    "                        .rename(\n",
    "                            columns={\n",
    "                                \"index\": \"Year\",\n",
    "                                \"variable\": \"Element\",\n",
    "                                \"value\": \"Distance from PM\",\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                    .mark_line()\n",
    "                    .encode(x=\"Year:N\", y=\"Distance from PM\", color=\"Element\")\n",
    "                    .properties(width=300, height=300, title=\"\")\n",
    "                )\n",
    "                chart.save(f\"results/{dataset}/top_10_distances.png\", scale_factor=4.0)\n",
    "                print(f\"Top 10 distances chart saved to results/{dataset}/top_10_distances.png\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating or saving top 10 distances chart: {e}\")\n",
    "\n",
    "            try:\n",
    "                # PCA analysis \n",
    "                # Convert long-form distance data to square symmetric matrix\n",
    "                espd_matrix = espd.pivot(index=\"Corpus 1\", columns=\"Corpus 2\", values=\"value\")\n",
    "\n",
    "                # Fill missing values, enforce symmetry, and zero diagonal\n",
    "                espd_matrix = espd_matrix.fillna(0)\n",
    "                espd_matrix = espd_matrix + espd_matrix.T\n",
    "                np.fill_diagonal(espd_matrix.values, 0)\n",
    "\n",
    "                # === PCA computation ===\n",
    "                pca = PCA(n_components=2)\n",
    "                pca_result = pca.fit_transform(espd_matrix)\n",
    "                explained_std = pca.explained_variance_ratio_\n",
    "\n",
    "                pca_df = pd.DataFrame(pca_result, columns=[\"PC1\", \"PC2\"])\n",
    "                pca_df[\"alias\"] = espd_matrix.index\n",
    "                 # === Save raw PCA plot ===\n",
    "                plt.figure(figsize=(6, 5))\n",
    "                plt.scatter(pca_df[\"PC1\"], pca_df[\"PC2\"], alpha=0.7)\n",
    "                for i, label in enumerate(pca_df[\"alias\"]):\n",
    "                    plt.text(pca_df.loc[i, \"PC1\"], pca_df.loc[i, \"PC2\"], label, fontsize=6, alpha=0.7)\n",
    "                plt.xlabel(f\"PC1 ({explained_std[0]:.2%} variance)\")\n",
    "                plt.ylabel(f\"PC2 ({explained_std[1]:.2%} variance)\")\n",
    "                plt.title(\"Raw PCA (Sockpuppet Distance Matrix)\")\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"results/{dataset}/pca_raw.png\", dpi=300)\n",
    "                plt.close()\n",
    "                print(f\"Raw PCA saved to results/{dataset}/pca_raw.png\")\n",
    "                merged_df=pca_df    \n",
    "                # === Save PCA_df to CSV ===\n",
    "                pca_df.to_csv(f\"results/{dataset}/pca_df.csv\", index=False)\n",
    "                print(f\"PCA DataFrame saved to results/{dataset}/pca_df.csv\")    \n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating or saving PCA chart: {e}\")\n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failure in generate_signatures: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline  \n",
    "# Reuse the functions from the basic example\n",
    "# clean_data, filter_data, calculate_summary, save_results\n",
    "\n",
    "def process_data(file_path, file_path2, ignore_columns, columns_to_keep,columns_to_remove,agg_column, var_name, value_name, output_path, output_dic, processing_type, sig_file,dataset,graph,top,sig_length,short_names):\n",
    "    \"\"\"\n",
    "    Pipeline function to load, unpivot, clean, and save data.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        file_path2 (str): Path to the second input CSV file.\n",
    "        ignore_columns (list): List of columns to ignore during unpivoting.\n",
    "        columns_to_keep (dict): Columns to keep and their new names.\n",
    "        agg_column (str): Column to aggregate by during unpivoting.\n",
    "        var_name (str): Name for the variable column after unpivoting.\n",
    "        value_name (str): Name for the value column after unpivoting.\n",
    "        output_path (str): Path to save the processed CSV file.\n",
    "        output_dic (dict, optional): Dictionary to save as a CSV file.\n",
    "    \"\"\"\n",
    "    df = load_data(file_path,columns_to_remove)\n",
    "    if df is None:\n",
    "        print(\"Pipeline aborted due to error in load_data.\")\n",
    "        return  # Stop the pipeline\n",
    "\n",
    "    df_unpivoted = unpivot_data(df, agg_column, var_name, value_name, ignore_columns,processing_type, file_path2)\n",
    "    if df_unpivoted is None:\n",
    "        print(\"Pipeline aborted due to error in unpivot_data.\")\n",
    "        return\n",
    "\n",
    "    df_cleaned ,entity_code_df = clean_data(df_unpivoted, columns_to_keep,short_names, dataset)\n",
    "    # print(df_cleaned  ) \n",
    "    if df_cleaned is None:\n",
    "        print(\"Pipeline aborted due to error in clean_data.\")  \n",
    "        return\n",
    " \n",
    "    save_results(df_cleaned,entity_code_df, output_path, output_dic)\n",
    "    print(\"Pipeline execution complete!\")\n",
    "\n",
    "    print (\"Generating signatures...\")\n",
    "    print (f\"sig_file: {sig_file}, dataset: {dataset}, graph: {graph}, top: {top}, sig_length: {sig_length}, var_name: {var_name}, value_name: {value_name}\")\n",
    "    generate_signatures(df_cleaned,entity_code_df,sig_file,dataset,graph,top,sig_length,var_name,value_name)  \n",
    "    print(\"signatures execution complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No columns to remove specified in the config file.\n",
      "Data successfully loaded from data/market/market.csv\n",
      "DataFrame shape: (1688, 15)\n",
      "                                Company  \\\n",
      "0                                Amazon   \n",
      "1  Foxconn (Hon Hai Precision Industry)   \n",
      "2                         Jingdong Mall   \n",
      "3                                   IBM   \n",
      "4                             Panasonic   \n",
      "\n",
      "                                   Company Path Country    Industry  \\\n",
      "0         https://companiesmarketcap.com/amazon     USA  Technology   \n",
      "1        https://companiesmarketcap.com/foxconn  Taiwan  Technology   \n",
      "2  https://companiesmarketcap.com/jingdong-mall   China  Technology   \n",
      "3            https://companiesmarketcap.com/ibm     USA  Technology   \n",
      "4      https://companiesmarketcap.com/panasonic   Japan  Technology   \n",
      "\n",
      "       Earnings  Employees     Marketcap    Net-Assets  Operating-Margin  \\\n",
      "0  4.073000e+10    1500000  1.966000e+12  2.018700e+11           -0.0115   \n",
      "1  6.160000e+09     826608  6.475000e+10  5.495000e+10            0.0255   \n",
      "2  4.910000e+09     450679  4.250000e+10  4.135000e+10            0.0255   \n",
      "3  8.690000e+09     288300  1.704200e+11  2.261000e+10            0.0308   \n",
      "4  3.170000e+09     232527  2.153000e+10  3.092000e+10            0.0487   \n",
      "\n",
      "   Pe-Ratio  Ps-Ratio       Revenue  Total-Assets    Total-Debt  \\\n",
      "0     96.50    3.4200  5.747800e+11  5.278500e+11  1.356100e+11   \n",
      "1     16.10    0.3255  1.983200e+11  1.284000e+11  3.073000e+10   \n",
      "2     13.70    0.2839  1.524800e+11  8.618000e+10  9.750000e+09   \n",
      "3     24.40    2.7600  6.185000e+10  1.352400e+11  5.993000e+10   \n",
      "4      7.18    0.3597  5.964000e+10  6.321000e+10  1.130000e+10   \n",
      "\n",
      "   Total-Liabilities  \n",
      "0       3.259700e+11  \n",
      "1       7.344000e+10  \n",
      "2       4.482000e+10  \n",
      "3       1.126200e+11  \n",
      "4       3.229000e+10  \n",
      "agg_column: Country, var_name: Industry, value_name: Marketcap, ignore_columns: ['Entity', 'Code']\n",
      "Unpivoted data shape:     document          element     Marketcap  Total_Per_Agg  \\\n",
      "0  Argentina      Electricity  9.400000e+08   8.106000e+10   \n",
      "1  Argentina             Food  0.000000e+00   8.106000e+10   \n",
      "2  Argentina        Insurance  0.000000e+00   8.106000e+10   \n",
      "3  Argentina          Oil&Gas  2.880000e+09   8.106000e+10   \n",
      "4  Argentina  Pharmaceuticals  0.000000e+00   8.106000e+10   \n",
      "\n",
      "   frequency_in_document  \n",
      "0               0.011596  \n",
      "1               0.000000  \n",
      "2               0.000000  \n",
      "3               0.035529  \n",
      "4               0.000000  \n",
      "Cleaning data...\n",
      "Saving results to: results/market/lvs.csv and results/market/dic.csv\n",
      "Saving results to: results/market/lvs.csv and results/market/dic.csv\n",
      "Data successfully saved to results/market/lvs.csv\n",
      "Dictionary successfully saved to results/market/lvs_dict.csv\n",
      "Pipeline execution complete!\n",
      "Generating signatures...\n",
      "sig_file: results/market/signatures.csv, dataset: market, graph: True, top: 25, sig_length: 200, var_name: Industry, value_name: Marketcap\n",
      "sig_file: results/market/signatures.csv, dataset: market, graph: True, top: 25, sig_length: 200, var_name: Industry, value_name: Marketcap\n",
      "   element_code          element  global_weight\n",
      "0             3          Oil&Gas       0.212548\n",
      "1             7       Technology       0.177180\n",
      "2             2        Insurance       0.151073\n",
      "3             1             Food       0.118242\n",
      "4             0      Electricity       0.104766\n",
      "5             6           Retail       0.102220\n",
      "6             4  Pharmaceuticals       0.092008\n",
      "7             5      Real Estate       0.041963\n",
      "\n",
      "\n",
      "Signatures successfully saved to results/market/signatures.csv\n",
      "Signatures with real names successfully saved to results/market/signatures_real_names.csv\n",
      "------------------------------\n",
      "Saved pivoted data for row Argentina to 'results/market/split_dataframes/row_Argentina.csv'\n",
      "Saved pivoted data for row Australia to 'results/market/split_dataframes/row_Australia.csv'\n",
      "Saved pivoted data for row Austria to 'results/market/split_dataframes/row_Austria.csv'\n",
      "Saved pivoted data for row Belgium to 'results/market/split_dataframes/row_Belgium.csv'\n",
      "Saved pivoted data for row Bermuda to 'results/market/split_dataframes/row_Bermuda.csv'\n",
      "Saved pivoted data for row Brazil to 'results/market/split_dataframes/row_Brazil.csv'\n",
      "Saved pivoted data for row Canada to 'results/market/split_dataframes/row_Canada.csv'\n",
      "Saved pivoted data for row Cayman Islands to 'results/market/split_dataframes/row_Cayman Islands.csv'\n",
      "Saved pivoted data for row Chile to 'results/market/split_dataframes/row_Chile.csv'\n",
      "Saved pivoted data for row China to 'results/market/split_dataframes/row_China.csv'\n",
      "Saved pivoted data for row Colombia to 'results/market/split_dataframes/row_Colombia.csv'\n",
      "Saved pivoted data for row Czech Republic to 'results/market/split_dataframes/row_Czech Republic.csv'\n",
      "Saved pivoted data for row Denmark to 'results/market/split_dataframes/row_Denmark.csv'\n",
      "Saved pivoted data for row Finland to 'results/market/split_dataframes/row_Finland.csv'\n",
      "Saved pivoted data for row France to 'results/market/split_dataframes/row_France.csv'\n",
      "Saved pivoted data for row Germany to 'results/market/split_dataframes/row_Germany.csv'\n",
      "Saved pivoted data for row Gibraltar to 'results/market/split_dataframes/row_Gibraltar.csv'\n",
      "Saved pivoted data for row Greece to 'results/market/split_dataframes/row_Greece.csv'\n",
      "Saved pivoted data for row Hong Kong to 'results/market/split_dataframes/row_Hong Kong.csv'\n",
      "Saved pivoted data for row Hungary to 'results/market/split_dataframes/row_Hungary.csv'\n",
      "Saved pivoted data for row Iceland to 'results/market/split_dataframes/row_Iceland.csv'\n",
      "Saved pivoted data for row India to 'results/market/split_dataframes/row_India.csv'\n",
      "Saved pivoted data for row Indonesia to 'results/market/split_dataframes/row_Indonesia.csv'\n",
      "Saved pivoted data for row Ireland to 'results/market/split_dataframes/row_Ireland.csv'\n",
      "Saved pivoted data for row Isle of Man to 'results/market/split_dataframes/row_Isle of Man.csv'\n",
      "Saved pivoted data for row Israel to 'results/market/split_dataframes/row_Israel.csv'\n",
      "Saved pivoted data for row Italy to 'results/market/split_dataframes/row_Italy.csv'\n",
      "Saved pivoted data for row Japan to 'results/market/split_dataframes/row_Japan.csv'\n",
      "Saved pivoted data for row Jordan to 'results/market/split_dataframes/row_Jordan.csv'\n",
      "Saved pivoted data for row Kazakhstan to 'results/market/split_dataframes/row_Kazakhstan.csv'\n",
      "Saved pivoted data for row Kuwait to 'results/market/split_dataframes/row_Kuwait.csv'\n",
      "Saved pivoted data for row Luxembourg to 'results/market/split_dataframes/row_Luxembourg.csv'\n",
      "Saved pivoted data for row Malaysia to 'results/market/split_dataframes/row_Malaysia.csv'\n",
      "Saved pivoted data for row Mexico to 'results/market/split_dataframes/row_Mexico.csv'\n",
      "Saved pivoted data for row Netherlands to 'results/market/split_dataframes/row_Netherlands.csv'\n",
      "Saved pivoted data for row New Zealand to 'results/market/split_dataframes/row_New Zealand.csv'\n",
      "Saved pivoted data for row Norway to 'results/market/split_dataframes/row_Norway.csv'\n",
      "Saved pivoted data for row Peru to 'results/market/split_dataframes/row_Peru.csv'\n",
      "Saved pivoted data for row Philippines to 'results/market/split_dataframes/row_Philippines.csv'\n",
      "Saved pivoted data for row Poland to 'results/market/split_dataframes/row_Poland.csv'\n",
      "Saved pivoted data for row Portugal to 'results/market/split_dataframes/row_Portugal.csv'\n",
      "Saved pivoted data for row Romania to 'results/market/split_dataframes/row_Romania.csv'\n",
      "Saved pivoted data for row Russia to 'results/market/split_dataframes/row_Russia.csv'\n",
      "Saved pivoted data for row Saudi Arabia to 'results/market/split_dataframes/row_Saudi Arabia.csv'\n",
      "Saved pivoted data for row Singapore to 'results/market/split_dataframes/row_Singapore.csv'\n",
      "Saved pivoted data for row South Africa to 'results/market/split_dataframes/row_South Africa.csv'\n",
      "Saved pivoted data for row South Korea to 'results/market/split_dataframes/row_South Korea.csv'\n",
      "Saved pivoted data for row Spain to 'results/market/split_dataframes/row_Spain.csv'\n",
      "Saved pivoted data for row Sweden to 'results/market/split_dataframes/row_Sweden.csv'\n",
      "Saved pivoted data for row Switzerland to 'results/market/split_dataframes/row_Switzerland.csv'\n",
      "Saved pivoted data for row Taiwan to 'results/market/split_dataframes/row_Taiwan.csv'\n",
      "Saved pivoted data for row Turkey to 'results/market/split_dataframes/row_Turkey.csv'\n",
      "Saved pivoted data for row UK to 'results/market/split_dataframes/row_UK.csv'\n",
      "Saved pivoted data for row USA to 'results/market/split_dataframes/row_USA.csv'\n",
      "Element list saved to results/market/list.txt\n",
      "Index(['Technology', 'Insurance', 'Food', 'Retail', 'Oil&Gas',\n",
      "       'Pharmaceuticals', 'Electricity', 'Real Estate'],\n",
      "      dtype='object')\n",
      "Element list saved to results/market/list.csv\n",
      "   element_code          element  global_weight\n",
      "0             3          Oil&Gas       0.212548\n",
      "1             7       Technology       0.177180\n",
      "2             2        Insurance       0.151073\n",
      "3             1             Food       0.118242\n",
      "4             0      Electricity       0.104766\n",
      "5             6           Retail       0.102220\n",
      "6             4  Pharmaceuticals       0.092008\n",
      "7             5      Real Estate       0.041963\n",
      "\n",
      "\n",
      "Sockpuppet distance matrix chart saved to results/market/sockpuppet_distance_matrix.png\n",
      "Top 10 distances chart saved to results/market/top_10_distances.png\n",
      "Raw PCA saved to results/market/pca_raw.png\n",
      "PCA DataFrame saved to results/market/pca_df.csv\n",
      "signatures execution complete!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1. Set up argument parser\n",
    "    #parser = argparse.ArgumentParser(description=\"Process data from a CSV file.\")\n",
    "    #parser.add_argument(\"--config\", help=\"Path to the config file\", default=\"config.toml\")\n",
    "    #args = parser.parse_args()\n",
    "    config_file_path = 'config_market.toml'  # Replace with your actual path\n",
    "\n",
    "    # 2. Read the config file\n",
    "    config = configparser.ConfigParser()\n",
    "    #config.read(args.config)\n",
    "    config.read(config_file_path)\n",
    "    # 3. Get parameters from the config\n",
    "    file_path = config.get(\"data\", \"file_path\")\n",
    "    file_path2 = config.get(\"data\", \"file_path2\")    \n",
    "    agg_column=config.get(\"proc\",\"agg_column\")\n",
    "    var_name=config.get(\"proc\",\"var_name\") \n",
    "    value_name=config.get(\"proc\",\"value_name\")  \n",
    "    processing_type = config.get(\"proc\",\"processing_type\")\n",
    "    columns_to_remove = config.get(\"proc\", \"columns_to_remove\").split(',') if config.has_option(\"proc\", \"columns_to_remove\") else []\n",
    "    # Convert to list if it's a comma-separated string\n",
    "    columns_to_remove = [col.strip() for col in columns_to_remove if col.strip()]  # Remove empty strings\n",
    "    # If the config file has no columns to remove, it will be an empty list \n",
    "    if not columns_to_remove:\n",
    "        print(\"No columns to remove specified in the config file.\")\n",
    "    else:\n",
    "        print(f\"Columns to remove: {columns_to_remove}\")    \n",
    "    #   \n",
    "    output_path = config.get(\"output\", \"output_path\") \n",
    "    output_dic = config.get(\"output\", \"output_dic\")  \n",
    "    sig_file = config.get(\"output\", \"sig_file\") \n",
    "    dataset = config.get(\"data\", \"dataset\")\n",
    "    graph = config.get(\"output\", \"graph\")\n",
    "    top = config.get(\"output\", \"top\")\n",
    "    sig_length = config.get(\"output\", \"sig_length\")\n",
    "    short_names  = config.get(\"output\", \"short_names\")\n",
    "    # constants \n",
    "    ignore_columns = ['Entity','Code']\n",
    "    columns_to_keep = ['document', 'element', 'frequency_in_document']  \n",
    "    #ignore_columns = []\n",
    "    # 4. Call the processing function\n",
    "    process_data(file_path,file_path2,ignore_columns,columns_to_keep,columns_to_remove,agg_column,var_name,value_name,output_path,output_dic,processing_type,sig_file,dataset,graph,top,sig_length,short_names) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
